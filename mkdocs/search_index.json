{
    "docs": [
        {
            "location": "/",
            "text": "Docker Scaler\n\u00b6\n\n\nThe goal of the \nDocker Scaler\n project is to provide a REST HTTP interface to scale services and nodes. It sends scaling alerts to \nAlertmanager\n which can be configured to notify you through email, Slack, etc.\n\n\nDocker Scaler\n examples can be found in the \nTutorials\n section located in the left-hand menu.\n\n\nPlease visit the \nConfiguration\n and \nUsage\n sections for details.",
            "title": "Home"
        },
        {
            "location": "/#docker-scaler",
            "text": "The goal of the  Docker Scaler  project is to provide a REST HTTP interface to scale services and nodes. It sends scaling alerts to  Alertmanager  which can be configured to notify you through email, Slack, etc.  Docker Scaler  examples can be found in the  Tutorials  section located in the left-hand menu.  Please visit the  Configuration  and  Usage  sections for details.",
            "title": "Docker Scaler"
        },
        {
            "location": "/service-scale/",
            "text": "Auto-Scaling With Docker Scaler And Instrumented Metrics\n\u00b6\n\n\nDocker Scaler\n provides an alternative to using Jenkins for service scaling shown in Docker Flow Monitor's \nauto-scaling tutorial\n. In this tutorial, we will construct a system that will scale a service based on response time. Here is an overview of the triggered events in our self-adapting system:\n\n\n\n\nThe \ngo-demo\n service response times becomes high.\n\n\nDocker Flow Monitor\n is querying the services' metrics, notices the high response times, and alerts the \nAlertmanager\n.\n\n\nThe Alertmanager is configured to forward the alert to \nDocker Scaler\n.\n\n\nDocker Scaler\n scales the service up.\n\n\n\n\nThis tutorial assumes you have Docker Machine version v0.8+ that includes Docker Engine v1.12+.\n\n\n\n\nInfo\n\n\nIf you are a Windows user, please run all the examples from \nGit Bash\n (installed through \nDocker for Windows\n). Also, make sure that your Git client is configured to check out the code \nAS-IS\n. Otherwise, Windows might change carriage returns to the Windows format.\n\n\n\n\nWe will be using \nSlack\n webhooks to notify us. Create a \nSlack\n channel and setup a webhook by consulting \nSlack's\n \nIncoming Webhook\n page. After obtaining a webhook URL set it as an environment variable:\n\n\nexport\n \nSLACK_WEBHOOK_URL\n=[\n...\n]\n\n\n\n\n\nSetting Up A Cluster\n\u00b6\n\n\n\n\nInfo\n\n\nFeel free to skip this section if you already have a Swarm cluster that can be used for this tutorial\n\n\n\n\nWe create a Swarm cluster consisting of three nodes created with Docker Machine.\n\n\ngit clone https://github.com/thomasjpfan/docker-scaler.git\n\n\ncd\n docker-scaler\n\n./scripts/ds-swarm.sh\n\n\neval\n \n$(\ndocker-machine env swarm-1\n)\n\n\n\n\n\nThe repo contains all the scripts and stack files needed throughout this tutorial. Next, we executed \nds-swarm.sh\n creating the cluster with \ndocker-machine\n. Finally, we used the \neval\n command to tell our local Docker client to use the remote Docker Engine \nswarm-1\n.\n\n\nDeploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)\n\u00b6\n\n\nFor convenience, we will use \nDocker Flow Proxy\n and \nDocker Flow Swarm Listener\n to get a single access point to the cluster.\n\n\ndocker network create -d overlay proxy\n\ndocker stack deploy \n\\\n\n    -c stacks/docker-flow-proxy-mem.yml \n\\\n\n    proxy\n\n\n\n\nPlease visit \nproxy.dockerflow.com\n and \nswarmlistener.dockerflow.com\n for details on the \nDocker Flow\n stack.\n\n\nDeploying Docker Scaler\n\u00b6\n\n\nWe can now deploy the \nDocker Scaler\n stack:\n\n\ndocker network create -d overlay scaler\n\ndocker stack deploy \n\\\n\n    -c stacks/docker-scaler-service-scale-tutorial.yml \n\\\n\n    scaler\n\n\n\n\nThis stack defines a single \nDocker Scaler\n service:\n\n\n...\n\n  \nservices\n:\n\n    \nscaler\n:\n\n      \nimage\n:\n \nthomasjpfan/docker-scaler\n\n      \nenvironment\n:\n\n        \n-\n \nALERTMANAGER_ADDRESS=http://alertmanager:9093\n\n        \n-\n \nSERVER_PREFIX=/scaler\n\n      \nvolumes\n:\n\n        \n-\n \n/var/run/docker.sock:/var/run/docker.sock\n\n      \nnetworks\n:\n\n        \n-\n \nscaler\n\n      \ndeploy\n:\n\n        \nreplicas\n:\n \n1\n\n        \nlabels\n:\n\n          \n-\n \ncom.df.notify=true\n\n          \n-\n \ncom.df.distribute=true\n\n          \n-\n \ncom.df.servicePath=/scaler\n\n          \n-\n \ncom.df.port=8080\n\n        \nplacement\n:\n\n            \nconstraints\n:\n \n[\nnode.role == manager\n]\n\n\n...\n\n\n\n\n\nThis definition constraints \nDocker Scaler\n to run on manager nodes and gives it access to the Docker socket, so that it can scale services in the cluster. The label \ncom.df.servicePath=/scaler\n and environement variable \nSERVER_PREFIX=/scaler\n allows us to interact with the \nscaler\n service.\n\n\nManually Scaling Services\n\u00b6\n\n\nFor this section, we deploy a simple sleeping service:\n\n\ndocker service create -d --replicas \n1\n \n\\\n\n  --name demo \n\\\n\n  -l com.df.scaleUpBy\n=\n3\n \n\\\n\n  -l com.df.scaleDownBy\n=\n2\n \n\\\n\n  -l com.df.scaleMin\n=\n1\n \n\\\n\n  -l com.df.scaleMax\n=\n7\n \n\\\n\n  alpine:3.6 sleep \n100000000000\n\n\n\n\n\nLabels \ncom.df.scaleUpby=3\n and \ncom.df.scaleDownby=2\n configures how many replicas to scale up and down by respectively. Labels \ncom.df.scaleMin=1\n and \ncom.df.scaleMax=7\n denote the mininum and maximum number of replicas. We manually scale up this service by sending a POST request:\n\n\ncurl -X POST http://\n$(\ndocker-machine ip swarm-1\n)\n/scaler/v1/scale-service -d \n\\\n\n\n'{\"groupLabels\": {\"scale\": \"up\", \"service\": \"demo\"}}'\n\n\n\n\n\nWe confirm that the \ndemo\n service have been scaled up by 3 from 1 replica to 4 replicas:\n\n\ndocker service ls -f \nname\n=\ndemo\n\n\n\n\nSimilarily, we manually scale down the service by sending:\n\n\ncurl -X POST http://\n$(\ndocker-machine ip swarm-1\n)\n/scaler/v1/scale-service -d \n\\\n\n\n'{\"groupLabels\": {\"scale\": \"down\", \"service\": \"demo\"}}'\n\n\n\n\n\nWe confirm that the \ndemo\n service has been scaled down by 2 from 4 replicas to 2 replicas:\n\n\ndocker service ls -f \nname\n=\ndemo\n\n\n\n\nBefore we continuing, remove the \ndemo\n service:\n\n\ndocker service rm demo\n\n\n\n\nDeploying Docker Flow Monitor and Alertmanager\n\u00b6\n\n\nThe next stack defines the \nDocker Flow Monitor\n and \nAlertmanager\n services. Before we deploy the stack, we defined our \nAlertmanager\n configuration as a Docker secret:\n\n\necho\n \n\"global:\n\n\n  slack_api_url: '\n$SLACK_WEBHOOK_URL\n'\n\n\nroute:\n\n\n  receiver: slack\n\n\n  group_by: [service, scale, type]\n\n\n  group_interval: 5m\n\n\n  repeat_interval: 5m\n\n\n  routes:\n\n\n  - match_re:\n\n\n      scale: up|down\n\n\n      type: service\n\n\n    receiver: scale\n\n\n  - match:\n\n\n      alertname: scale_service\n\n\n    group_by: [alertname]\n\n\n    group_interval: 15s\n\n\n    group_wait: 5s\n\n\n    receiver: slack-scaler\n\n\n\nreceivers:\n\n\n  - name: 'slack'\n\n\n    slack_configs:\n\n\n      - send_resolved: true\n\n\n        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is in danger!'\n\n\n        title_link: 'http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts'\n\n\n        text: '{{ .CommonAnnotations.summary }}'\n\n\n  - name: 'slack-scaler'\n\n\n    slack_configs:\n\n\n      - title: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.request }}'\n\n\n        color: '{{ if eq .CommonLabels.status \\\"error\\\" }}danger{{ else }}good{{ end }}'\n\n\n        title_link: 'http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts'\n\n\n        text: '{{ .CommonAnnotations.summary }}'\n\n\n  - name: 'scale'\n\n\n    webhook_configs:\n\n\n      - send_resolved: false\n\n\n        url: 'http://scaler:8080/v1/scale-service'\n\n\n\"\n \n|\n docker secret create alert_manager_config -\n\n\nThis configuration groups alerts by their \nservice\n, \nscale\n, and \ntype\n labels. The \nroutes\n section defines a \nmatch_re\n entry, that directs scale alerts to the \nscale\n reciever. Another route is configured to direct alerts from the \nscaler\n service to the \nslack-scaler\n receiver. Now we deploy the monitor stack:\n\n\ndocker network create -d overlay monitor\n\n\nDOMAIN\n=\n$(\ndocker-machine ip swarm-1\n)\n \n\\\n\n    docker stack deploy \n\\\n\n    -c stacks/docker-flow-monitor-slack.yml \n\\\n\n    monitor\n\n\n\n\nThe \nalert-manager\n service is configured to read the \nalert_manager_config\n secret in the stack definition as follows:\n\n\n...\n\n  \nalert-manager\n:\n\n    \nimage\n:\n \nprom/alertmanager\n\n    \nnetworks\n:\n\n      \n-\n \nmonitor\n\n      \n-\n \nscaler\n\n    \nsecrets\n:\n\n      \n-\n \nalert_manager_config\n\n    \ncommand\n:\n \n-config.file=/run/secrets/alert_manager_config -storage.path=/alertmanager\n\n\n...\n\n\n\n\n\nWith access to the \nscaler\n network, \nalert-manager\n can send scaling requests to the \nscaler\n service. For information about the Docker Flow Monitor stack can be found in its \ndocumentation\n.\n\n\nLet us confirm that the \nmonitor\n stack is up and running:\n\n\ndocker stack ps monitor\n\n\n\n\nPlease wait a few moments for all the replicas to have the status \nrunning\n. After the \nmonitor\n stack is up and running, we can start deploying the \ngo-demo_main\n service!\n\n\nDeploying Instrumented Service\n\u00b6\n\n\nThe \ngo-demo\n service already exposes response time metrics with labels for \nDocker Flow Monitor\n to scrape. We deploy the service to be scaled based on the response time metrics:\n\n\ndocker stack deploy \n\\\n\n    -c stacks/go-demo-instrument-alert-short.yml \n\\\n\n    go-demo\n\n\n\n\nThe full stack definition can be found at \ngo-demo-instrument-alert-short.yml\n. We will focus on the service labels for the \ngo-demo_main\n service relating to scaling and alerting:\n\n\nmain\n:\n\n  \n...\n\n  \ndeploy\n:\n\n    \n...\n\n    \nlabels\n:\n\n      \n...\n\n      \n- com.df.alertName.1=resptimeabove\n\n      \n- com.df.alertIf.1=@resp_time_above:0.1,5m,0.99\n\n      \n- com.df.alertName.2=resptimebelow_unless_resptimeabove\n\n      \n- com.df.alertIf.2=sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.025\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) > 0.75 unless sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.1\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) < 0.99\n\n      \n- com.df.alertLabels.2=receiver=system,service=go-demo_main,scale=down,type=service\n\n      \n- com.df.alertAnnotations.2=summary=Response time of service go-demo_main is below 0.025 and not above 0.1\n\n    \n...\n\n\n\nThe \nalertName\n, \nalertIf\n and \nalertFor\n labels uses the \nAlertIf Parameter Shortcuts\n for creating Prometheus expressions that firing alerts. The alert \ncom.df.alertIf.1=@resp_time_above:0.1,5m,0.99\n fires when the number of responses above 0.1 seconds in the last 5 minutes consist of more than 99% of all responses. The second alert fires when the number of response below 0.025 seconds in the last 5 minutes consist of more than 75% unless alert 1 is firing.\n\n\nWe can view the alerts generated by these labels:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts\"\n\n\n\n\n\nLet's confirm that the go-demo stack is up-and-running:\n\n\ndocker stack ps -f desired-state\n=\nrunning go-demo\n\n\n\n\nThere should be three replicas of the \ngo-demo_main\n service and one replica of the \ngo-demo_db\n service. Please wait for all replicas to be up and running.\n\n\nWe can confirm that \nDocker Flow Monitor\n is monitoring the \ngo-demo\n replicas:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/targets\"\n\n\n\n\n\nThere should be two or three targets depending on whether Prometheus already sent the alert to de-scale the service.\n\n\nAutomatically Scaling Services\n\u00b6\n\n\nLet's go back to the Prometheus' alert screen:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts\"\n\n\n\n\n\nBy this time, the \ngodemo_main_resptimebelow_unless_resptimeabove\n alert should be red due to having no requests sent to \ngo-demo\n. The Alertmanager recieves this alert and sends a \nPOST\n request to the \nscaler\n service to scale down \ngo-demo\n. The label \ncom.df.scaleDownBy\n on \ngo-demo_main\n is set to 1 thus the number of replicas goes from 4 to 3.\n\n\nLet's look at the logs of \nscaler\n:\n\n\ndocker service logs scaler_scaler\n\n\n\n\nThere should be a log message that states \nScaling go-demo_main from 4 to 3 replicas (min: 2)\n. We can check that this happened:\n\n\ndocker service ls -f \nname\n=\ngo-demo_main\n\n\n\n\nThe output should be similar to the following:\n\n\nNAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          \n3\n/3                 vfarcic/go-demo:latest\n\n\n\n\nPlease visit your channel and you should see a Slack notification stating that \ngo-demo_main\n has scaled from 4 to 3 replicas.\n\n\nLet's see what happens when response times of the service becomes high by sending requests that will result in high response times:\n\n\nfor\n i in \n{\n1\n..30\n}\n;\n \ndo\n\n    \nDELAY\n=\n$\n[\n \n$RANDOM\n % \n6000\n \n]\n\n    curl \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/demo/hello?delay=\n$DELAY\n\"\n\n\ndone\n\n\n\n\n\nLet's look at the alerts:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts\"\n\n\n\n\n\nThe \ngodemo_main_resptimeabove\n turned red indicating that the threshold is reached. \nAlertmanager\n receives the alert, sends a \nPOST\n request to the \nscaler\n service, and \ndocker-scaler\n scales \ngo-demo_main\n up by the value of \ncom.df.scaleUpBy\n. In this case, the value of \ncom.df.scaleUpBy\n is two. Let's look at the logs of \ndocker-scaler\n:\n\n\ndocker service logs scaler_scaler\n\n\n\n\nThere should be a log message that states \nScaling go-demo_main from 3 to 5 replicas (max: 7)\n. This message is also sent through Slack to notify us of this scaling event.\n\n\nWe can confirm that the number of replicas indeed scaled to three by querying the stack processes:\n\n\ndocker service ls -f \nname\n=\ngo-demo_main\n\n\n\n\nThe output should look similar to the following:\n\n\nNAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          5/5                 vfarcic/go-demo:latest\n\n\n\n\nWhat Now?\n\u00b6\n\n\nWe just went through a simple example of a system that automatically scales and de-scales services. Feel free to add additional metrics and services to this self-adapting system to customize it to your needs.\n\n\nPlease remove the demo cluster we created and free your resources:\n\n\ndocker-machine rm -f swarm-1 swarm-2 swarm-3",
            "title": "Service Scaling"
        },
        {
            "location": "/service-scale/#auto-scaling-with-docker-scaler-and-instrumented-metrics",
            "text": "Docker Scaler  provides an alternative to using Jenkins for service scaling shown in Docker Flow Monitor's  auto-scaling tutorial . In this tutorial, we will construct a system that will scale a service based on response time. Here is an overview of the triggered events in our self-adapting system:   The  go-demo  service response times becomes high.  Docker Flow Monitor  is querying the services' metrics, notices the high response times, and alerts the  Alertmanager .  The Alertmanager is configured to forward the alert to  Docker Scaler .  Docker Scaler  scales the service up.   This tutorial assumes you have Docker Machine version v0.8+ that includes Docker Engine v1.12+.   Info  If you are a Windows user, please run all the examples from  Git Bash  (installed through  Docker for Windows ). Also, make sure that your Git client is configured to check out the code  AS-IS . Otherwise, Windows might change carriage returns to the Windows format.   We will be using  Slack  webhooks to notify us. Create a  Slack  channel and setup a webhook by consulting  Slack's   Incoming Webhook  page. After obtaining a webhook URL set it as an environment variable:  export   SLACK_WEBHOOK_URL =[ ... ]",
            "title": "Auto-Scaling With Docker Scaler And Instrumented Metrics"
        },
        {
            "location": "/service-scale/#setting-up-a-cluster",
            "text": "Info  Feel free to skip this section if you already have a Swarm cluster that can be used for this tutorial   We create a Swarm cluster consisting of three nodes created with Docker Machine.  git clone https://github.com/thomasjpfan/docker-scaler.git cd  docker-scaler\n\n./scripts/ds-swarm.sh eval   $( docker-machine env swarm-1 )   The repo contains all the scripts and stack files needed throughout this tutorial. Next, we executed  ds-swarm.sh  creating the cluster with  docker-machine . Finally, we used the  eval  command to tell our local Docker client to use the remote Docker Engine  swarm-1 .",
            "title": "Setting Up A Cluster"
        },
        {
            "location": "/service-scale/#deploying-docker-flow-proxy-dfp-and-docker-flow-swarm-listener-dfsl",
            "text": "For convenience, we will use  Docker Flow Proxy  and  Docker Flow Swarm Listener  to get a single access point to the cluster.  docker network create -d overlay proxy\n\ndocker stack deploy  \\ \n    -c stacks/docker-flow-proxy-mem.yml  \\ \n    proxy  Please visit  proxy.dockerflow.com  and  swarmlistener.dockerflow.com  for details on the  Docker Flow  stack.",
            "title": "Deploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)"
        },
        {
            "location": "/service-scale/#deploying-docker-scaler",
            "text": "We can now deploy the  Docker Scaler  stack:  docker network create -d overlay scaler\n\ndocker stack deploy  \\ \n    -c stacks/docker-scaler-service-scale-tutorial.yml  \\ \n    scaler  This stack defines a single  Docker Scaler  service:  ... \n   services : \n     scaler : \n       image :   thomasjpfan/docker-scaler \n       environment : \n         -   ALERTMANAGER_ADDRESS=http://alertmanager:9093 \n         -   SERVER_PREFIX=/scaler \n       volumes : \n         -   /var/run/docker.sock:/var/run/docker.sock \n       networks : \n         -   scaler \n       deploy : \n         replicas :   1 \n         labels : \n           -   com.df.notify=true \n           -   com.df.distribute=true \n           -   com.df.servicePath=/scaler \n           -   com.df.port=8080 \n         placement : \n             constraints :   [ node.role == manager ]  ...   This definition constraints  Docker Scaler  to run on manager nodes and gives it access to the Docker socket, so that it can scale services in the cluster. The label  com.df.servicePath=/scaler  and environement variable  SERVER_PREFIX=/scaler  allows us to interact with the  scaler  service.",
            "title": "Deploying Docker Scaler"
        },
        {
            "location": "/service-scale/#manually-scaling-services",
            "text": "For this section, we deploy a simple sleeping service:  docker service create -d --replicas  1   \\ \n  --name demo  \\ \n  -l com.df.scaleUpBy = 3   \\ \n  -l com.df.scaleDownBy = 2   \\ \n  -l com.df.scaleMin = 1   \\ \n  -l com.df.scaleMax = 7   \\ \n  alpine:3.6 sleep  100000000000   Labels  com.df.scaleUpby=3  and  com.df.scaleDownby=2  configures how many replicas to scale up and down by respectively. Labels  com.df.scaleMin=1  and  com.df.scaleMax=7  denote the mininum and maximum number of replicas. We manually scale up this service by sending a POST request:  curl -X POST http:// $( docker-machine ip swarm-1 ) /scaler/v1/scale-service -d  \\  '{\"groupLabels\": {\"scale\": \"up\", \"service\": \"demo\"}}'   We confirm that the  demo  service have been scaled up by 3 from 1 replica to 4 replicas:  docker service ls -f  name = demo  Similarily, we manually scale down the service by sending:  curl -X POST http:// $( docker-machine ip swarm-1 ) /scaler/v1/scale-service -d  \\  '{\"groupLabels\": {\"scale\": \"down\", \"service\": \"demo\"}}'   We confirm that the  demo  service has been scaled down by 2 from 4 replicas to 2 replicas:  docker service ls -f  name = demo  Before we continuing, remove the  demo  service:  docker service rm demo",
            "title": "Manually Scaling Services"
        },
        {
            "location": "/service-scale/#deploying-docker-flow-monitor-and-alertmanager",
            "text": "The next stack defines the  Docker Flow Monitor  and  Alertmanager  services. Before we deploy the stack, we defined our  Alertmanager  configuration as a Docker secret:  echo   \"global:    slack_api_url: ' $SLACK_WEBHOOK_URL '  route:    receiver: slack    group_by: [service, scale, type]    group_interval: 5m    repeat_interval: 5m    routes:    - match_re:        scale: up|down        type: service      receiver: scale    - match:        alertname: scale_service      group_by: [alertname]      group_interval: 15s      group_wait: 5s      receiver: slack-scaler  receivers:    - name: 'slack'      slack_configs:        - send_resolved: true          title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is in danger!'          title_link: 'http:// $( docker-machine ip swarm-1 ) /monitor/alerts'          text: '{{ .CommonAnnotations.summary }}'    - name: 'slack-scaler'      slack_configs:        - title: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.request }}'          color: '{{ if eq .CommonLabels.status \\\"error\\\" }}danger{{ else }}good{{ end }}'          title_link: 'http:// $( docker-machine ip swarm-1 ) /monitor/alerts'          text: '{{ .CommonAnnotations.summary }}'    - name: 'scale'      webhook_configs:        - send_resolved: false          url: 'http://scaler:8080/v1/scale-service'  \"   |  docker secret create alert_manager_config - \nThis configuration groups alerts by their  service ,  scale , and  type  labels. The  routes  section defines a  match_re  entry, that directs scale alerts to the  scale  reciever. Another route is configured to direct alerts from the  scaler  service to the  slack-scaler  receiver. Now we deploy the monitor stack:  docker network create -d overlay monitor DOMAIN = $( docker-machine ip swarm-1 )   \\ \n    docker stack deploy  \\ \n    -c stacks/docker-flow-monitor-slack.yml  \\ \n    monitor  The  alert-manager  service is configured to read the  alert_manager_config  secret in the stack definition as follows:  ... \n   alert-manager : \n     image :   prom/alertmanager \n     networks : \n       -   monitor \n       -   scaler \n     secrets : \n       -   alert_manager_config \n     command :   -config.file=/run/secrets/alert_manager_config -storage.path=/alertmanager  ...   With access to the  scaler  network,  alert-manager  can send scaling requests to the  scaler  service. For information about the Docker Flow Monitor stack can be found in its  documentation .  Let us confirm that the  monitor  stack is up and running:  docker stack ps monitor  Please wait a few moments for all the replicas to have the status  running . After the  monitor  stack is up and running, we can start deploying the  go-demo_main  service!",
            "title": "Deploying Docker Flow Monitor and Alertmanager"
        },
        {
            "location": "/service-scale/#deploying-instrumented-service",
            "text": "The  go-demo  service already exposes response time metrics with labels for  Docker Flow Monitor  to scrape. We deploy the service to be scaled based on the response time metrics:  docker stack deploy  \\ \n    -c stacks/go-demo-instrument-alert-short.yml  \\ \n    go-demo  The full stack definition can be found at  go-demo-instrument-alert-short.yml . We will focus on the service labels for the  go-demo_main  service relating to scaling and alerting:  main : \n   ... \n   deploy : \n     ... \n     labels : \n       ... \n       - com.df.alertName.1=resptimeabove \n       - com.df.alertIf.1=@resp_time_above:0.1,5m,0.99 \n       - com.df.alertName.2=resptimebelow_unless_resptimeabove \n       - com.df.alertIf.2=sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.025\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) > 0.75 unless sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.1\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) < 0.99 \n       - com.df.alertLabels.2=receiver=system,service=go-demo_main,scale=down,type=service \n       - com.df.alertAnnotations.2=summary=Response time of service go-demo_main is below 0.025 and not above 0.1 \n     ...  \nThe  alertName ,  alertIf  and  alertFor  labels uses the  AlertIf Parameter Shortcuts  for creating Prometheus expressions that firing alerts. The alert  com.df.alertIf.1=@resp_time_above:0.1,5m,0.99  fires when the number of responses above 0.1 seconds in the last 5 minutes consist of more than 99% of all responses. The second alert fires when the number of response below 0.025 seconds in the last 5 minutes consist of more than 75% unless alert 1 is firing.  We can view the alerts generated by these labels:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/alerts\"   Let's confirm that the go-demo stack is up-and-running:  docker stack ps -f desired-state = running go-demo  There should be three replicas of the  go-demo_main  service and one replica of the  go-demo_db  service. Please wait for all replicas to be up and running.  We can confirm that  Docker Flow Monitor  is monitoring the  go-demo  replicas:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/targets\"   There should be two or three targets depending on whether Prometheus already sent the alert to de-scale the service.",
            "title": "Deploying Instrumented Service"
        },
        {
            "location": "/service-scale/#automatically-scaling-services",
            "text": "Let's go back to the Prometheus' alert screen:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/alerts\"   By this time, the  godemo_main_resptimebelow_unless_resptimeabove  alert should be red due to having no requests sent to  go-demo . The Alertmanager recieves this alert and sends a  POST  request to the  scaler  service to scale down  go-demo . The label  com.df.scaleDownBy  on  go-demo_main  is set to 1 thus the number of replicas goes from 4 to 3.  Let's look at the logs of  scaler :  docker service logs scaler_scaler  There should be a log message that states  Scaling go-demo_main from 4 to 3 replicas (min: 2) . We can check that this happened:  docker service ls -f  name = go-demo_main  The output should be similar to the following:  NAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated           3 /3                 vfarcic/go-demo:latest  Please visit your channel and you should see a Slack notification stating that  go-demo_main  has scaled from 4 to 3 replicas.  Let's see what happens when response times of the service becomes high by sending requests that will result in high response times:  for  i in  { 1 ..30 } ;   do \n     DELAY = $ [   $RANDOM  %  6000   ] \n    curl  \"http:// $( docker-machine ip swarm-1 ) /demo/hello?delay= $DELAY \"  done   Let's look at the alerts:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/alerts\"   The  godemo_main_resptimeabove  turned red indicating that the threshold is reached.  Alertmanager  receives the alert, sends a  POST  request to the  scaler  service, and  docker-scaler  scales  go-demo_main  up by the value of  com.df.scaleUpBy . In this case, the value of  com.df.scaleUpBy  is two. Let's look at the logs of  docker-scaler :  docker service logs scaler_scaler  There should be a log message that states  Scaling go-demo_main from 3 to 5 replicas (max: 7) . This message is also sent through Slack to notify us of this scaling event.  We can confirm that the number of replicas indeed scaled to three by querying the stack processes:  docker service ls -f  name = go-demo_main  The output should look similar to the following:  NAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          5/5                 vfarcic/go-demo:latest",
            "title": "Automatically Scaling Services"
        },
        {
            "location": "/service-scale/#what-now",
            "text": "We just went through a simple example of a system that automatically scales and de-scales services. Feel free to add additional metrics and services to this self-adapting system to customize it to your needs.  Please remove the demo cluster we created and free your resources:  docker-machine rm -f swarm-1 swarm-2 swarm-3",
            "title": "What Now?"
        },
        {
            "location": "/aws-node-scale/",
            "text": "AWS Node Scaling\n\u00b6\n\n\nDocker Scaler\n includes endpoints to scale nodes on AWS. In this tutorial, we will construct a system that will scale up worker nodes based on memory usage. This tutorial uses \nAWS CLI\n to communicate with AWS and \njq\n to parse json responses from the CLI.\n\n\n\n\nInfo\n\n\nIf you are a Windows user, please run all the examples from \nGit Bash\n (installed through \nDocker for Windows\n). Also, make sure that your Git client is configured to check out the code \nAS-IS\n. Otherwise, Windows might change carriage returns to the Windows format.\n\n\n\n\nSetting up Current Environment\n\u00b6\n\n\nWe will be using \nSlack\n webhooks to notify us. First, we create a \nSlack\n workspace and setup a webhook by consulting \nSlack's\n \nIncoming Webhook\n page. After obtaining a webhook URL set it as an environment variable:\n\n\nexport\n \nSLACK_WEBHOOK_URL\n=[\n...\n]\n\n\n\n\n\nThe AWS CLI is configured by setting the following environment variables:\n\n\nexport\n \nAWS_ACCESS_KEY_ID\n=[\n...\n]\n\n\nexport\n \nAWS_SECRET_ACCESS_KEY\n=[\n...\n]\n\n\nexport\n \nAWS_DEFAULT_REGION\n=\nus-east-1\n\n\n\n\nThe \nIAM Policies\n required for this tutorial are \ncloudformation:*\n, \nsqs:*\n, \niam:*\n, \nec2:*\n, \nlambda:*\n, \ndynamodb:*\n, \n\"autoscaling:*\n, and \nelasticfilesystem:*\n.\n\n\nFor convenience, we define the \nSTACK_NAME\n to be the name of our AWS stack, \nKEY_FILE\n to be the path to the ssh AWS identity file, and \nKEY_NAME\n as the key's name on AWS.\n\n\nexport\n \nSTACK_NAME\n=\ndevops22\n\nexport\n \nKEY_FILE\n=\ndevops22.pem \n# Location of pem file\n\n\nexport\n \nKEY_NAME\n=\ndevops22\n\n\n\n\nSetting Up An AWS Cluster\n\u00b6\n\n\nUsing AWS Cloudformation, we will create a cluster of three master ndoes:\n\n\naws cloudformation create-stack \n\\\n\n    --template-url https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl \n\\\n\n    --capabilities CAPABILITY_IAM \n\\\n\n    --stack-name \n$STACK_NAME\n \n\\\n\n    --parameters \n\\\n\n    \nParameterKey\n=\nManagerSize,ParameterValue\n=\n3\n \n\\\n\n    \nParameterKey\n=\nClusterSize,ParameterValue\n=\n0\n \n\\\n\n    \nParameterKey\n=\nKeyName,ParameterValue\n=\n$KEY_NAME\n \n\\\n\n    \nParameterKey\n=\nEnableSystemPrune,ParameterValue\n=\nyes \n\\\n\n    \nParameterKey\n=\nEnableCloudWatchLogs,ParameterValue\n=\nno \n\\\n\n    \nParameterKey\n=\nEnableCloudStorEfs,ParameterValue\n=\nyes \n\\\n\n    \nParameterKey\n=\nManagerInstanceType,ParameterValue\n=\nt2.micro \n\\\n\n    \nParameterKey\n=\nInstanceType,ParameterValue\n=\nt2.micro\n\n\n\n\nWe can check if the cluster came online by running:\n\n\naws cloudformation describe-stacks \n\\\n\n    --stack-name \n$STACK_NAME\n \n|\n \n\\\n\n    jq -r \n\".Stacks[0].StackStatus\"\n\n\n\n\n\nPlease wait till the output of this command is \nCREATE_COMPLETE\n before continuing.\n\n\nSetting up the AWS Environment\n\u00b6\n\n\nWe need to log into a manager node to issue Docker commands and interact with our Docker swarm. To setup the manager shell environmental, we will define variables in our current shell and transfer them to the manager node.\n\n\nWe save the cluster dns to an environment variable \nCLUSTER_DNS\n:\n\n\nCLUSTER_DNS\n=\n$(\naws cloudformation \n\\\n\n    describe-stacks \n\\\n\n    --stack-name \n$STACK_NAME\n \n|\n \n\\\n\n    jq -r \n\".Stacks[0].Outputs[] | \\\n\n\n    select(.OutputKey==\\\"DefaultDNSTarget\\\")\\\n\n\n    .OutputValue\"\n)\n\n\n\n\n\nWe set the environment variable \nCLUSTER_IP\n to the public ip of one of the manager nodes:\n\n\nCLUSTER_IP\n=\n$(\naws ec2 describe-instances \n\\\n\n    \n|\n jq -r \n\".Reservations[] \\\n\n\n    .Instances[] \\\n\n\n    | select(.SecurityGroups[].GroupName \\\n\n\n    | contains(\\\"\n$STACK_NAME\n-ManagerVpcSG\\\"))\\\n\n\n    .PublicIpAddress\"\n \n\\\n\n    \n|\n tail -n \n1\n)\n\n\n\n\n\nWe save the the manager and worker autoscaling group names:\n\n\nWORKER_ASG\n=\n$(\naws autoscaling \n\\\n\n    describe-auto-scaling-groups \n\\\n\n    \n|\n jq -r \n\".AutoScalingGroups[] \\\n\n\n    | select(.AutoScalingGroupName \\\n\n\n    | startswith(\\\"\n$STACK_NAME\n-NodeAsg-\\\"))\\\n\n\n    .AutoScalingGroupName\"\n)\n\n\n\nMANAGER_ASG\n=\n$(\naws autoscaling \n\\\n\n    describe-auto-scaling-groups \n\\\n\n    \n|\n jq -r \n\".AutoScalingGroups[] \\\n\n\n    | select(.AutoScalingGroupName \\\n\n\n    | startswith(\\\"\n$STACK_NAME\n-ManagerAsg-\\\"))\\\n\n\n    .AutoScalingGroupName\"\n)\n\n\n\n\n\nWe clone the \nDocker Scaler\n repo and transfer the stacks folder\n\n\ngit clone https://github.com/thomasjpfan/docker-scaler.git\n\nscp -i \n$KEY_FILE\n -rp docker-scaler/stacks docker@\n$CLUSTER_IP\n:~\n\n\n\n\nUsing ssh, we can transfer the environment variables into a file on the manager node:\n\n\necho\n \n\"\n\n\nexport CLUSTER_DNS=\n$CLUSTER_DNS\n\n\nexport AWS_ACCESS_KEY_ID=\n$AWS_ACCESS_KEY_ID\n\n\nexport AWS_SECRET_ACCESS_KEY=\n$AWS_SECRET_ACCESS_KEY\n\n\nexport AWS_DEFAULT_REGION=\n$AWS_DEFAULT_REGION\n\n\nexport WORKER_ASG=\n$WORKER_ASG\n\n\nexport MANAGER_ASG=\n$MANAGER_ASG\n\n\nexport SLACK_WEBHOOK_URL=\n$SLACK_WEBHOOK_URL\n\n\n\"\n \n|\n ssh -i \n$KEY_FILE\n docker@\n$CLUSTER_IP\n \n\"cat > env\"\n\n\n\n\n\nFinally, we can log into the manager node and source the environment variables:\n\n\nssh -i \n$KEY_FILE\n docker@\n$CLUSTER_IP\n\n\n\nsource\n env\n\n\n\n\nDeploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)\n\u00b6\n\n\nFor convenience, we will use \nDocker Flow Proxy\n and \nDocker Flow Swarm Listener\n to get a single access point to the cluster.\n\n\necho\n \n\"admin:admin\"\n \n|\n docker secret \n\\\n\n    create dfp_users_admin -\n\ndocker network create -d overlay proxy\n\ndocker stack deploy \n\\\n\n    -c stacks/docker-flow-proxy-aws.yml \n\\\n\n    proxy\n\n\n\n\nPlease visit \nproxy.dockerflow.com\n and \nswarmlistener.dockerflow.com\n for details on the \nDocker Flow\n stack.\n\n\nDeploying Docker Scaler\n\u00b6\n\n\nTo allow \nDocker Scaler\n to access AWS, the credientials are stored in a Docker secret:\n\n\necho\n \n\"\n\n\nexport AWS_ACCESS_KEY_ID=\n$AWS_ACCESS_KEY_ID\n\n\nexport AWS_SECRET_ACCESS_KEY=\n$AWS_SECRET_ACCESS_KEY\n\n\n\"\n \n|\n docker secret create aws -\n\n\n\n\nWe can now deploy the \nDocker Scaler\n stack:\n\n\ndocker network create -d overlay scaler\n\ndocker stack deploy \n\\\n\n    -c stacks/scaler-aws-tutorial.yml \n\\\n\n    scaler\n\n\n\n\nThis stack defines a single \nDocker Scaler\n service. Focusing on the environment variables set by the compose file:\n\n\n...\n\n  \nservices\n:\n\n    \nscaler\n:\n\n      \nimage\n:\n \nthomasjpfan/scaler\n\n      \nenvironment\n:\n\n        \n-\n \nALERTMANAGER_ADDRESS=http://alert-manager:9093\n\n        \n-\n \nNODE_SCALER_BACKEND=aws\n\n        \n-\n \nAWS_MANAGER_ASG=${MANAGER_ASG}\n\n        \n-\n \nAWS_WORKER_ASG=${WORKER_ASG}\n\n        \n-\n \nAWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n\n        \n-\n \nSERVER_PREFIX=/scaler\n\n      \nlabels\n:\n\n        \n-\n \ncom.df.notify=true\n\n        \n-\n \ncom.df.distribute=true\n\n        \n-\n \ncom.df.servicePath=/scaler\n\n        \n-\n \ncom.df.port=8080\n\n      \nsecrets\n:\n\n        \n-\n \naws\n\n\n...\n\n\n\n\n\nThe \nNODE_SCALER_BACKEND\n must be set to \naws\n to configure \nDocker Scaler\n to scale nodes on AWS. The label \ncom.df.servicePath=/scaler\n and environment variable \nSERVER_PREFIX\n opens up the \nscaler\n service to public REST calls. For this tutorial, we open this path to explore manually scaling nodes.\n\n\nManually Scaling Nodes\n\u00b6\n\n\nBefore node scaling, we will deploy a simple sleeping service:\n\n\ndocker service create -d --replicas \n6\n \n\\\n\n  -l com.df.reschedule\n=\ntrue\n \n\\\n\n  --name demo \n\\\n\n  alpine:3.6 sleep \n100000000000\n\n\n\n\n\nThe \ncom.df.reschedule=true\n label signals to \nDocker Scaler\n that this service is allowed for rescheduling after node scaling.\n\n\nThe original cluster started out with three manager nodes. We now scale up the worker nodes by one be issuing a \nPOST\n request:\n\n\ncurl -X POST http://\n$CLUSTER_DNS\n/scaler/v1/scale-nodes\n\\?\nby\n\\=\n1\n\\&\ntype\n\\=\nworker -d \n\\\n\n\n'{\"groupLabels\": {\"scale\": \"up\"}}'\n\n\n\n\n\nThe parameters \nby=1\n and \ntype=worker\n tell the service to scale worker nodes up by \n1\n. Inside the json request body, the \nscale\n value is set to \nup\n to denote scaling up. To scale nodes down just set the value to \ndown\n. We can check the number of nodes by running:\n\n\ndocker node ls\n\n\n\n\nThe output should be similar to the following (node ids are discarded):\n\n\nHOSTNAME                        STATUS              AVAILABILITY        MANAGER STATUS\nip-172-31-4-44.ec2.internal     Ready               Active              Reachable\nip-172-31-17-200.ec2.internal   Ready               Active              Reachable\nip-172-31-20-95.ec2.internal    Ready               Active\nip-172-31-44-49.ec2.internal    Ready               Active              Leader\n\n\n\n\nIf there are still three nodes, wait a few more minutes and try the command again. \nDocker Scaler\n waits for the new node to come up and reschedules services that are labeled \ncom.df.reschedule=true\n. We look at the processes running on the new worker node:\n\n\nNEW_NODE=$(docker node ls -f role=worker -q)\n\ndocker node ps $NEW_NODE\n\n\n\n\nThe output should include some instances of the \ndemo\n service, showing that the some of the instances has been place on the new node. We will now move on to implementing a system for automatic scaling!\n\n\nDeploying Docker Flow Monitor and Alertmanager\n\u00b6\n\n\nThe next stack defines the \nDocker Flow Monitor\n and \nAlertmanager\n services. Before we deploy the stack, we defined our \nAlertmanager\n configuration as a Docker secret:\n\n\necho\n \n\"global:\n\n\n  slack_api_url: '\n$SLACK_WEBHOOK_URL\n'\n\n\nroute:\n\n\n  group_interval: 30m\n\n\n  repeat_interval: 30m\n\n\n  receiver: slack\n\n\n  group_by: [service, scale, type]\n\n\n  routes:\n\n\n    - match_re:\n\n\n        scale: up|down\n\n\n        type: node\n\n\n      receiver: 'scale-nodes'\n\n\n    - match_re:\n\n\n        alertname: scale_service|reschedule_service|scale_nodes\n\n\n      group_by: [alertname]\n\n\n      group_wait: 5s\n\n\n      group_interval: 15s\n\n\n      receiver: slack-scaler\n\n\n\nreceivers:\n\n\n  - name: 'slack'\n\n\n    slack_configs:\n\n\n      - send_resolved: true\n\n\n        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is in danger!'\n\n\n        title_link: 'http://\n$CLUSTER_DNS\n/monitor/alerts'\n\n\n        text: '{{ .CommonAnnotations.summary }}'\n\n\n  - name: 'slack-scaler'\n\n\n    slack_configs:\n\n\n      - title: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.request }}'\n\n\n        color: '{{ if eq .CommonLabels.status \\\"error\\\" }}danger{{ else }}good{{ end }}'\n\n\n        title_link: 'http://\n$CLUSTER_DNS\n/monitor/alerts'\n\n\n        text: '{{ .CommonAnnotations.summary }}'\n\n\n  - name: 'scale-nodes'\n\n\n    webhook_configs:\n\n\n      - send_resolved: false\n\n\n        url: 'http://scaler:8080/v1/scale-nodes?by=1&type=worker'\n\n\n\"\n \n|\n docker secret create alert_manager_config -\n\n\n\n\nThis configuration groups alerts by their \nservice\n, \nscale\n, and \ntype\n labels. The \nroutes\n section defines a \nmatch_re\n entry, that directs scale alerts to the \nscale-nodes\n reciever. The second route is configured to direct alerts from the \nscaler\n service to the \nslack-scaler\n receiver. The \nscale-nodes\n receivers \nurl\n is given parameters \nby=1\n to denote how many nodes to scale down or up by, and \ntype=worker\n to only scale worker nodes.\n\n\ndocker network create -d overlay monitor\n\n\nDOMAIN\n=\n$CLUSTER_DNS\n \n\\\n\n    docker stack deploy \n\\\n\n    -c stacks/docker-flow-monitor-aws.yml \n\\\n\n    monitor\n\n\n\n\nLet us confirm that the \nmonitor\n stack is up and running:\n\n\ndocker stack ps monitor\n\n\n\n\nPlease wait a few moments for all the replicas to have the status \nrunning\n. After the \nmonitor\n stack is up and running, we can move on to deploying node exporters.\n\n\nDeploying Node Exporters\n\u00b6\n\n\nThe node exporters are used to display metrics about each nodes for \nDocker Flow Monitor\n to scrap. To deploy the exporters stack run:\n\n\ndocker stack deploy \n\\\n\n  -c stacks/exporters-aws.yml \n\\\n\n  exporter\n\n\n\n\nWe will focus on the service labels for the \nnode-exporter-manager\n and \nnode-exporter-worker\n services:\n\n\n...\n\n\nservices\n:\n\n  \n...\n\n  \nnode-exporter-manager\n:\n\n    \n...\n\n    \ndeploy\n:\n\n      \nlabels\n:\n\n        \n...\n\n        \n- com.df.alertName.1=node_mem_limit_total_above\n\n        \n- com.df.alertIf.1=@node_mem_limit_total_above:0.95\n\n        \n- com.df.alertLabels.1=receiver=system,scale=no,service=exporter_node-exporter-manager,type=node\n\n        \n- com.df.alertFor.1=30s\n\n        \n...\n\n  \nnode-exporter-worker\n:\n\n    \n...\n\n    \ndeploy\n:\n\n      \nlabels\n:\n\n        \n...\n\n        \n- com.df.alertName.1=node_mem_limit_total_above\n\n        \n- com.df.alertIf.1=@node_mem_limit_total_above:0.95\n\n        \n- com.df.alertFor.1=30s\n\n        \n- com.df.alertName.2=node_mem_limit_total_below\n\n        \n- com.df.alertIf.2=@node_mem_limit_total_below:0.05\n\n        \n- com.df.alertFor.2=30s\n\n\n...\n\n\n\n\n\nThese labels use \nAlertIf Parameter Shortcuts\n for creating Prometheus expressions that firing alerts.\n\n\nThe \nnode-exporter-manager\n has an \nalertIf\n label of \nnode_mem_limit_total_above:0.95\n, which will fire when the total fractional memory of the all manager nodes is above 95%. Setting one of the \nalertLabels\n to \nscale=no\n prevents autoscaling and sends a notification to Slack. The \nnode-exproter-worker\n has an \nalertIf\n label of \nnode_mem_limit_total_above:0.95\n which will fire when the total fractional memory of all worker nodes is above 95%. Similiary, the \nnode_mem_limit_total_below:0.01\n fires when the total fractional memory is below 5%. These values for memory alerts are extreme to prevent the alerts from firing. We will change these labels to explore what happens when they fire.\n\n\nFor example, we trigger alert 1 on \nnode-exporter-manager\n by changing its alert label:\n\n\ndocker service update \n\\\n\n  --label-add \n\"com.df.alertIf.1=@node_mem_limit_total_above:0.05\"\n \n\\\n\n  exporter_node-exporter-manager\n\n\n\n\nAfter the alert is fired, we can see a \nSlack\n notification stating \nTotal memory of the nodes is over 0.05\n. Before we continue, we return the alert back to before:\n\n\ndocker service update \n\\\n\n  --label-add \n\"com.df.alertIf.1=@node_mem_limit_total_above:0.95\"\n \n\\\n\n  exporter_node-exporter-manager\n\n\n\n\nAutomaticall Scaling Nodes\n\u00b6\n\n\nWe trigger alert 1 on \nnode-exporter-worker\n by setting the \nnode_mem_limit_total_above\n limit to 5%:\n\n\ndocker service update \n\\\n\n  --label-add \n\"com.df.alertIf.1=@node_mem_limit_total_above:0.05\"\n \n\\\n\n  exporter_node-exporter-manager\n\n\n\n\nAfter a few moments, the alert will fire and trigger \nscaler\n to scale worker nodes up by one. We confirm that there is now five nodes by running:\n\n\ndocker node ls\n\n\n\n\nAfter the node comes up, \nscaler\n will also reschedule services with label, \ncom.df.reschedule=true\n. During this process, \nSlack\n notifications were sent to inform us of each step. Before triggering the alert 2, we return alert 1 back to before:\n\n\ndocker service update \n\\\n\n  --label-add \n\"com.df.alertIf.1=@node_mem_limit_total_above:0.95\"\n \n\\\n\n  exporter_node-exporter-manager\n\n\n\n\nWe trigger the condition for scaling down a node by setting \nnode_mem_limit_total_below\n limit to 95%:\n\n\ndocker service update \n\\\n\n  --label-add \n\"com.df.alertIf.2=@node_mem_limit_total_below:0.95\"\n \n\\\n\n  exporter_node-exporter-manager\n\n\n\n\nAfter a few moments, the alert will fire and trigger \nscaler\n to scale worker nodes down by one. We confirm that there is now four nodes by running:\n\n\ndocker node ls\n\n\n\n\nWhat Now?\n\u00b6\n\n\nWe just went through a simple example of a system that automatically scales and de-scales nodes. Feel free to add additional metrics and services to this self-adapting system to customize it to your needs.\n\n\nPlease remove the AWS cluster we created and free your resources:\n\n\n aws cloudformation delete-stack \\\n    --stack-name $STACK_NAME\n\n\n\n\nYou can navigate to \nAWS Cloudformation\n to confirm that your stack has been removed.",
            "title": "AWS Node Scaling"
        },
        {
            "location": "/aws-node-scale/#aws-node-scaling",
            "text": "Docker Scaler  includes endpoints to scale nodes on AWS. In this tutorial, we will construct a system that will scale up worker nodes based on memory usage. This tutorial uses  AWS CLI  to communicate with AWS and  jq  to parse json responses from the CLI.   Info  If you are a Windows user, please run all the examples from  Git Bash  (installed through  Docker for Windows ). Also, make sure that your Git client is configured to check out the code  AS-IS . Otherwise, Windows might change carriage returns to the Windows format.",
            "title": "AWS Node Scaling"
        },
        {
            "location": "/aws-node-scale/#setting-up-current-environment",
            "text": "We will be using  Slack  webhooks to notify us. First, we create a  Slack  workspace and setup a webhook by consulting  Slack's   Incoming Webhook  page. After obtaining a webhook URL set it as an environment variable:  export   SLACK_WEBHOOK_URL =[ ... ]   The AWS CLI is configured by setting the following environment variables:  export   AWS_ACCESS_KEY_ID =[ ... ]  export   AWS_SECRET_ACCESS_KEY =[ ... ]  export   AWS_DEFAULT_REGION = us-east-1  The  IAM Policies  required for this tutorial are  cloudformation:* ,  sqs:* ,  iam:* ,  ec2:* ,  lambda:* ,  dynamodb:* ,  \"autoscaling:* , and  elasticfilesystem:* .  For convenience, we define the  STACK_NAME  to be the name of our AWS stack,  KEY_FILE  to be the path to the ssh AWS identity file, and  KEY_NAME  as the key's name on AWS.  export   STACK_NAME = devops22 export   KEY_FILE = devops22.pem  # Location of pem file  export   KEY_NAME = devops22",
            "title": "Setting up Current Environment"
        },
        {
            "location": "/aws-node-scale/#setting-up-an-aws-cluster",
            "text": "Using AWS Cloudformation, we will create a cluster of three master ndoes:  aws cloudformation create-stack  \\ \n    --template-url https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl  \\ \n    --capabilities CAPABILITY_IAM  \\ \n    --stack-name  $STACK_NAME   \\ \n    --parameters  \\ \n     ParameterKey = ManagerSize,ParameterValue = 3   \\ \n     ParameterKey = ClusterSize,ParameterValue = 0   \\ \n     ParameterKey = KeyName,ParameterValue = $KEY_NAME   \\ \n     ParameterKey = EnableSystemPrune,ParameterValue = yes  \\ \n     ParameterKey = EnableCloudWatchLogs,ParameterValue = no  \\ \n     ParameterKey = EnableCloudStorEfs,ParameterValue = yes  \\ \n     ParameterKey = ManagerInstanceType,ParameterValue = t2.micro  \\ \n     ParameterKey = InstanceType,ParameterValue = t2.micro  We can check if the cluster came online by running:  aws cloudformation describe-stacks  \\ \n    --stack-name  $STACK_NAME   |   \\ \n    jq -r  \".Stacks[0].StackStatus\"   Please wait till the output of this command is  CREATE_COMPLETE  before continuing.",
            "title": "Setting Up An AWS Cluster"
        },
        {
            "location": "/aws-node-scale/#setting-up-the-aws-environment",
            "text": "We need to log into a manager node to issue Docker commands and interact with our Docker swarm. To setup the manager shell environmental, we will define variables in our current shell and transfer them to the manager node.  We save the cluster dns to an environment variable  CLUSTER_DNS :  CLUSTER_DNS = $( aws cloudformation  \\ \n    describe-stacks  \\ \n    --stack-name  $STACK_NAME   |   \\ \n    jq -r  \".Stacks[0].Outputs[] | \\      select(.OutputKey==\\\"DefaultDNSTarget\\\")\\      .OutputValue\" )   We set the environment variable  CLUSTER_IP  to the public ip of one of the manager nodes:  CLUSTER_IP = $( aws ec2 describe-instances  \\ \n     |  jq -r  \".Reservations[] \\      .Instances[] \\      | select(.SecurityGroups[].GroupName \\      | contains(\\\" $STACK_NAME -ManagerVpcSG\\\"))\\      .PublicIpAddress\"   \\ \n     |  tail -n  1 )   We save the the manager and worker autoscaling group names:  WORKER_ASG = $( aws autoscaling  \\ \n    describe-auto-scaling-groups  \\ \n     |  jq -r  \".AutoScalingGroups[] \\      | select(.AutoScalingGroupName \\      | startswith(\\\" $STACK_NAME -NodeAsg-\\\"))\\      .AutoScalingGroupName\" )  MANAGER_ASG = $( aws autoscaling  \\ \n    describe-auto-scaling-groups  \\ \n     |  jq -r  \".AutoScalingGroups[] \\      | select(.AutoScalingGroupName \\      | startswith(\\\" $STACK_NAME -ManagerAsg-\\\"))\\      .AutoScalingGroupName\" )   We clone the  Docker Scaler  repo and transfer the stacks folder  git clone https://github.com/thomasjpfan/docker-scaler.git\n\nscp -i  $KEY_FILE  -rp docker-scaler/stacks docker@ $CLUSTER_IP :~  Using ssh, we can transfer the environment variables into a file on the manager node:  echo   \"  export CLUSTER_DNS= $CLUSTER_DNS  export AWS_ACCESS_KEY_ID= $AWS_ACCESS_KEY_ID  export AWS_SECRET_ACCESS_KEY= $AWS_SECRET_ACCESS_KEY  export AWS_DEFAULT_REGION= $AWS_DEFAULT_REGION  export WORKER_ASG= $WORKER_ASG  export MANAGER_ASG= $MANAGER_ASG  export SLACK_WEBHOOK_URL= $SLACK_WEBHOOK_URL  \"   |  ssh -i  $KEY_FILE  docker@ $CLUSTER_IP   \"cat > env\"   Finally, we can log into the manager node and source the environment variables:  ssh -i  $KEY_FILE  docker@ $CLUSTER_IP  source  env",
            "title": "Setting up the AWS Environment"
        },
        {
            "location": "/aws-node-scale/#deploying-docker-flow-proxy-dfp-and-docker-flow-swarm-listener-dfsl",
            "text": "For convenience, we will use  Docker Flow Proxy  and  Docker Flow Swarm Listener  to get a single access point to the cluster.  echo   \"admin:admin\"   |  docker secret  \\ \n    create dfp_users_admin -\n\ndocker network create -d overlay proxy\n\ndocker stack deploy  \\ \n    -c stacks/docker-flow-proxy-aws.yml  \\ \n    proxy  Please visit  proxy.dockerflow.com  and  swarmlistener.dockerflow.com  for details on the  Docker Flow  stack.",
            "title": "Deploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)"
        },
        {
            "location": "/aws-node-scale/#deploying-docker-scaler",
            "text": "To allow  Docker Scaler  to access AWS, the credientials are stored in a Docker secret:  echo   \"  export AWS_ACCESS_KEY_ID= $AWS_ACCESS_KEY_ID  export AWS_SECRET_ACCESS_KEY= $AWS_SECRET_ACCESS_KEY  \"   |  docker secret create aws -  We can now deploy the  Docker Scaler  stack:  docker network create -d overlay scaler\n\ndocker stack deploy  \\ \n    -c stacks/scaler-aws-tutorial.yml  \\ \n    scaler  This stack defines a single  Docker Scaler  service. Focusing on the environment variables set by the compose file:  ... \n   services : \n     scaler : \n       image :   thomasjpfan/scaler \n       environment : \n         -   ALERTMANAGER_ADDRESS=http://alert-manager:9093 \n         -   NODE_SCALER_BACKEND=aws \n         -   AWS_MANAGER_ASG=${MANAGER_ASG} \n         -   AWS_WORKER_ASG=${WORKER_ASG} \n         -   AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \n         -   SERVER_PREFIX=/scaler \n       labels : \n         -   com.df.notify=true \n         -   com.df.distribute=true \n         -   com.df.servicePath=/scaler \n         -   com.df.port=8080 \n       secrets : \n         -   aws  ...   The  NODE_SCALER_BACKEND  must be set to  aws  to configure  Docker Scaler  to scale nodes on AWS. The label  com.df.servicePath=/scaler  and environment variable  SERVER_PREFIX  opens up the  scaler  service to public REST calls. For this tutorial, we open this path to explore manually scaling nodes.",
            "title": "Deploying Docker Scaler"
        },
        {
            "location": "/aws-node-scale/#manually-scaling-nodes",
            "text": "Before node scaling, we will deploy a simple sleeping service:  docker service create -d --replicas  6   \\ \n  -l com.df.reschedule = true   \\ \n  --name demo  \\ \n  alpine:3.6 sleep  100000000000   The  com.df.reschedule=true  label signals to  Docker Scaler  that this service is allowed for rescheduling after node scaling.  The original cluster started out with three manager nodes. We now scale up the worker nodes by one be issuing a  POST  request:  curl -X POST http:// $CLUSTER_DNS /scaler/v1/scale-nodes \\? by \\= 1 \\& type \\= worker -d  \\  '{\"groupLabels\": {\"scale\": \"up\"}}'   The parameters  by=1  and  type=worker  tell the service to scale worker nodes up by  1 . Inside the json request body, the  scale  value is set to  up  to denote scaling up. To scale nodes down just set the value to  down . We can check the number of nodes by running:  docker node ls  The output should be similar to the following (node ids are discarded):  HOSTNAME                        STATUS              AVAILABILITY        MANAGER STATUS\nip-172-31-4-44.ec2.internal     Ready               Active              Reachable\nip-172-31-17-200.ec2.internal   Ready               Active              Reachable\nip-172-31-20-95.ec2.internal    Ready               Active\nip-172-31-44-49.ec2.internal    Ready               Active              Leader  If there are still three nodes, wait a few more minutes and try the command again.  Docker Scaler  waits for the new node to come up and reschedules services that are labeled  com.df.reschedule=true . We look at the processes running on the new worker node:  NEW_NODE=$(docker node ls -f role=worker -q)\n\ndocker node ps $NEW_NODE  The output should include some instances of the  demo  service, showing that the some of the instances has been place on the new node. We will now move on to implementing a system for automatic scaling!",
            "title": "Manually Scaling Nodes"
        },
        {
            "location": "/aws-node-scale/#deploying-docker-flow-monitor-and-alertmanager",
            "text": "The next stack defines the  Docker Flow Monitor  and  Alertmanager  services. Before we deploy the stack, we defined our  Alertmanager  configuration as a Docker secret:  echo   \"global:    slack_api_url: ' $SLACK_WEBHOOK_URL '  route:    group_interval: 30m    repeat_interval: 30m    receiver: slack    group_by: [service, scale, type]    routes:      - match_re:          scale: up|down          type: node        receiver: 'scale-nodes'      - match_re:          alertname: scale_service|reschedule_service|scale_nodes        group_by: [alertname]        group_wait: 5s        group_interval: 15s        receiver: slack-scaler  receivers:    - name: 'slack'      slack_configs:        - send_resolved: true          title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is in danger!'          title_link: 'http:// $CLUSTER_DNS /monitor/alerts'          text: '{{ .CommonAnnotations.summary }}'    - name: 'slack-scaler'      slack_configs:        - title: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.request }}'          color: '{{ if eq .CommonLabels.status \\\"error\\\" }}danger{{ else }}good{{ end }}'          title_link: 'http:// $CLUSTER_DNS /monitor/alerts'          text: '{{ .CommonAnnotations.summary }}'    - name: 'scale-nodes'      webhook_configs:        - send_resolved: false          url: 'http://scaler:8080/v1/scale-nodes?by=1&type=worker'  \"   |  docker secret create alert_manager_config -  This configuration groups alerts by their  service ,  scale , and  type  labels. The  routes  section defines a  match_re  entry, that directs scale alerts to the  scale-nodes  reciever. The second route is configured to direct alerts from the  scaler  service to the  slack-scaler  receiver. The  scale-nodes  receivers  url  is given parameters  by=1  to denote how many nodes to scale down or up by, and  type=worker  to only scale worker nodes.  docker network create -d overlay monitor DOMAIN = $CLUSTER_DNS   \\ \n    docker stack deploy  \\ \n    -c stacks/docker-flow-monitor-aws.yml  \\ \n    monitor  Let us confirm that the  monitor  stack is up and running:  docker stack ps monitor  Please wait a few moments for all the replicas to have the status  running . After the  monitor  stack is up and running, we can move on to deploying node exporters.",
            "title": "Deploying Docker Flow Monitor and Alertmanager"
        },
        {
            "location": "/aws-node-scale/#deploying-node-exporters",
            "text": "The node exporters are used to display metrics about each nodes for  Docker Flow Monitor  to scrap. To deploy the exporters stack run:  docker stack deploy  \\ \n  -c stacks/exporters-aws.yml  \\ \n  exporter  We will focus on the service labels for the  node-exporter-manager  and  node-exporter-worker  services:  ...  services : \n   ... \n   node-exporter-manager : \n     ... \n     deploy : \n       labels : \n         ... \n         - com.df.alertName.1=node_mem_limit_total_above \n         - com.df.alertIf.1=@node_mem_limit_total_above:0.95 \n         - com.df.alertLabels.1=receiver=system,scale=no,service=exporter_node-exporter-manager,type=node \n         - com.df.alertFor.1=30s \n         ... \n   node-exporter-worker : \n     ... \n     deploy : \n       labels : \n         ... \n         - com.df.alertName.1=node_mem_limit_total_above \n         - com.df.alertIf.1=@node_mem_limit_total_above:0.95 \n         - com.df.alertFor.1=30s \n         - com.df.alertName.2=node_mem_limit_total_below \n         - com.df.alertIf.2=@node_mem_limit_total_below:0.05 \n         - com.df.alertFor.2=30s  ...   These labels use  AlertIf Parameter Shortcuts  for creating Prometheus expressions that firing alerts.  The  node-exporter-manager  has an  alertIf  label of  node_mem_limit_total_above:0.95 , which will fire when the total fractional memory of the all manager nodes is above 95%. Setting one of the  alertLabels  to  scale=no  prevents autoscaling and sends a notification to Slack. The  node-exproter-worker  has an  alertIf  label of  node_mem_limit_total_above:0.95  which will fire when the total fractional memory of all worker nodes is above 95%. Similiary, the  node_mem_limit_total_below:0.01  fires when the total fractional memory is below 5%. These values for memory alerts are extreme to prevent the alerts from firing. We will change these labels to explore what happens when they fire.  For example, we trigger alert 1 on  node-exporter-manager  by changing its alert label:  docker service update  \\ \n  --label-add  \"com.df.alertIf.1=@node_mem_limit_total_above:0.05\"   \\ \n  exporter_node-exporter-manager  After the alert is fired, we can see a  Slack  notification stating  Total memory of the nodes is over 0.05 . Before we continue, we return the alert back to before:  docker service update  \\ \n  --label-add  \"com.df.alertIf.1=@node_mem_limit_total_above:0.95\"   \\ \n  exporter_node-exporter-manager",
            "title": "Deploying Node Exporters"
        },
        {
            "location": "/aws-node-scale/#automaticall-scaling-nodes",
            "text": "We trigger alert 1 on  node-exporter-worker  by setting the  node_mem_limit_total_above  limit to 5%:  docker service update  \\ \n  --label-add  \"com.df.alertIf.1=@node_mem_limit_total_above:0.05\"   \\ \n  exporter_node-exporter-manager  After a few moments, the alert will fire and trigger  scaler  to scale worker nodes up by one. We confirm that there is now five nodes by running:  docker node ls  After the node comes up,  scaler  will also reschedule services with label,  com.df.reschedule=true . During this process,  Slack  notifications were sent to inform us of each step. Before triggering the alert 2, we return alert 1 back to before:  docker service update  \\ \n  --label-add  \"com.df.alertIf.1=@node_mem_limit_total_above:0.95\"   \\ \n  exporter_node-exporter-manager  We trigger the condition for scaling down a node by setting  node_mem_limit_total_below  limit to 95%:  docker service update  \\ \n  --label-add  \"com.df.alertIf.2=@node_mem_limit_total_below:0.95\"   \\ \n  exporter_node-exporter-manager  After a few moments, the alert will fire and trigger  scaler  to scale worker nodes down by one. We confirm that there is now four nodes by running:  docker node ls",
            "title": "Automaticall Scaling Nodes"
        },
        {
            "location": "/aws-node-scale/#what-now",
            "text": "We just went through a simple example of a system that automatically scales and de-scales nodes. Feel free to add additional metrics and services to this self-adapting system to customize it to your needs.  Please remove the AWS cluster we created and free your resources:   aws cloudformation delete-stack \\\n    --stack-name $STACK_NAME  You can navigate to  AWS Cloudformation  to confirm that your stack has been removed.",
            "title": "What Now?"
        },
        {
            "location": "/usage/",
            "text": "Usage\n\u00b6\n\n\nDocker Scaler\n is controlled by sending HTTP requests to \n[SCALER_IP]:[SCALER_PORT]\n\n\nScaling Services\n\u00b6\n\n\nThis request queries docker service labels: \ncom.df.scaleMin\n, \ncom.df.scaleMax\n, \ncom.df.scaleDownBy\n, \ncom.df.scaleUpBy\n to determine how much to scale the service. Please see \nconfiguration\n for details.\n\n\n\n\n\n\nURL:\n\n    \n/v1/scale-service\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nRequest Body:\n\n\n\n\n\n\n{\n\n    \n\"groupLabels\"\n:\n \n{\n\n        \n\"scale\"\n:\n \n\"up\"\n,\n\n        \n\"service\"\n:\n \n\"example_web\"\n\n    \n}\n\n\n}\n\n\n\n\n\nThe \nservice\n value is the name of the service to scale. The \nscale\n value accepts \nup\n for scaling up and \ndown\n for scaling down.\n\n\nRescheduling All Services\n\u00b6\n\n\nThis request only reschedule services with label: \ncom.df.reschedule=true\n. See \nConfiguration\n to change this default.\n\n\n\n\n\n\nURL:\n\n    \n/v1/reschedule-services\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nRescheduling One Service\n\u00b6\n\n\nThis request only reschedule target service with label: \ncom.df.reschedule=true\n. See \nConfiguration\n to change this default.\n\n\n\n\n\n\nURL:\n\n    \n/v1/reschedule-service\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nQuery Parameters:\n\n\n\n\n\n\n\n\n\n\n\n\nQuery\n\n\nDescription\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nservice\n\n\nName of service to reschedule\n\n\nyes\n\n\n\n\n\n\n\n\nScaling Nodes\n\u00b6\n\n\n\n\n\n\nURL:\n\n    \n/v1/scale-nodes\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nQuery Parameters:\n\n\n\n\n\n\n\n\n\n\n\n\nQuery\n\n\nDescription\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nby\n\n\nThe number of nodes to scale up or down by\n\n\nyes\n\n\n\n\n\n\n\n\n\n\nRequest Body:\n\n\n\n\n{\n\n    \n\"groupLabels\"\n:\n \n{\n\n        \n\"scale\"\n:\n \n\"down\"\n,\n\n    \n}\n\n\n}\n\n\n\n\n\nThe \nscale\n value accepts \nup\n for scaling up and \ndown\n for scaling down.",
            "title": "Usage"
        },
        {
            "location": "/usage/#usage",
            "text": "Docker Scaler  is controlled by sending HTTP requests to  [SCALER_IP]:[SCALER_PORT]",
            "title": "Usage"
        },
        {
            "location": "/usage/#scaling-services",
            "text": "This request queries docker service labels:  com.df.scaleMin ,  com.df.scaleMax ,  com.df.scaleDownBy ,  com.df.scaleUpBy  to determine how much to scale the service. Please see  configuration  for details.    URL: \n     /v1/scale-service    Method: \n     POST    Request Body:    { \n     \"groupLabels\" :   { \n         \"scale\" :   \"up\" , \n         \"service\" :   \"example_web\" \n     }  }   The  service  value is the name of the service to scale. The  scale  value accepts  up  for scaling up and  down  for scaling down.",
            "title": "Scaling Services"
        },
        {
            "location": "/usage/#rescheduling-all-services",
            "text": "This request only reschedule services with label:  com.df.reschedule=true . See  Configuration  to change this default.    URL: \n     /v1/reschedule-services    Method: \n     POST",
            "title": "Rescheduling All Services"
        },
        {
            "location": "/usage/#rescheduling-one-service",
            "text": "This request only reschedule target service with label:  com.df.reschedule=true . See  Configuration  to change this default.    URL: \n     /v1/reschedule-service    Method: \n     POST    Query Parameters:       Query  Description  Required      service  Name of service to reschedule  yes",
            "title": "Rescheduling One Service"
        },
        {
            "location": "/usage/#scaling-nodes",
            "text": "URL: \n     /v1/scale-nodes    Method: \n     POST    Query Parameters:       Query  Description  Required      by  The number of nodes to scale up or down by  yes      Request Body:   { \n     \"groupLabels\" :   { \n         \"scale\" :   \"down\" , \n     }  }   The  scale  value accepts  up  for scaling up and  down  for scaling down.",
            "title": "Scaling Nodes"
        },
        {
            "location": "/configuration/",
            "text": "Configuring Docker Scaler\n\u00b6\n\n\nDocker Scaler\n can be configured through Docker enivonment variables and/or by creating a new image based on \nthomasjpfan/docker-scaler\n\n\nService Scaling Environment Variables\n\u00b6\n\n\n\n\nTip\n\n\nThe \nDocker Scaler\n container can be configured through envionment variables\n\n\n\n\nThe following environment variables can be used to configure the \nDocker Scaler\n relating to service scaling.\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSERVER_PREFIX\n\n\nCustom prefix for REST endpoint.\nDefault:\n \n/\n\n\n\n\n\n\nMIN_SCALE_LABEL\n\n\nService label key with value representing the minimum number of replicas.\nDefault:\n \ncom.df.scaleMin\n\n\n\n\n\n\nMAX_SCALE_LABEL\n\n\nService label key with value representing the maximum number of replicas.\nDefault:\n \ncom.df.scaleMax\n\n\n\n\n\n\nSCALE_DOWN_BY_LABEL\n\n\nService label key with value representing the number of replicas to scale down by.\nDefault:\n \ncom.df.scaleDownBy\n\n\n\n\n\n\nSCALE_UP_BY_LABEL\n\n\nService label key with value representing the number of replicas to scale up by.\nDefault:\n \ncom.df.scaleUpBy\n\n\n\n\n\n\nALERT_SCALE_MAX\n\n\nSend alert to alertmanager when trying to scale up service already at max replicas.\nDefault:\n true\n\n\n\n\n\n\nDEFAULT_MIN_REPLICAS\n\n\nDefault minimum number of replicas for a service.\nDefault:\n 1\n\n\n\n\n\n\nDEFAULT_MAX_REPLICAS\n\n\nDefault maximum number of replicas for a service.\nDefault:\n 5\n\n\n\n\n\n\nDEFAULT_SCALE_SERVICE_DOWN_BY\n\n\nDefault number of replicas to scale service down by.\nDefault:\n 1\n\n\n\n\n\n\nDEFAULT_SCALE_SERVICE_UP_BY\n\n\nDefault number of replicas to scale service up by.\nDefault:\n 1\n\n\n\n\n\n\nALERTMANAGER_ADDRESS\n\n\nAddress for alertmanager.\nDefault:\n \nhttp://alertmanager:9093\n\n\n\n\n\n\nALERT_TIMEOUT\n\n\nAlert timeout duration (seconds).\nDefault:\n 10\n\n\n\n\n\n\nRESCHEDULE_FILTER_LABEL\n\n\nServices with this label will be rescheduled after node scaling.\nDefault:\n \ncom.df.reschedule=true\"\n\n\n\n\n\n\nRESCHEDULE_TICKER_INTERVAL\n\n\nDuration to wait when checking for nodes to come up (seconds).\nDefault:\n 20\n\n\n\n\n\n\nRESCHEDULE_TIMEOUT\n\n\nTime to wait for nodes to come up during rescheduling (seconds).\nDefault:\n 1000\n\n\n\n\n\n\nRESCHEDULE_ENV_KEY\n\n\nKey for env variable when rescheduling services.\nDefault:\n \nRESCHEDULE_DATE\n\n\n\n\n\n\n\n\nNode Scaling Environment Variables\n\u00b6\n\n\nThe following environment variables can be used to configure the \nDocker Scaler\n relating to node scaling.\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNODE_SCALER_BACKEND\n\n\nBackend of node backend.\nAccepted Values:\n [none, aws]\nDefault:\n none\n\n\n\n\n\n\nDEFAULT_MIN_MANAGER_NODES\n\n\nMiniumum number of manager nodes.\nDefault:\n 3\n\n\n\n\n\n\nDEFAULT_MAX_MANAGER_NODES\n\n\nMaximum number of manager nodes.\nDefault:\n 7\n\n\n\n\n\n\nDEFAULT_MIN_WORKER_NODES\n\n\nMiniumum number of worker nodes.\nDefault:\n 1\n\n\n\n\n\n\nDEFAULT_MAX_WORKER_NODES\n\n\nMaximum number of worker nodes.\nDefault:\n 5\n\n\n\n\n\n\n\n\nAWS Node Scaling Envronment Variables\n\u00b6\n\n\nThe following environment variables can be used to configure the \nDocker Scaler\n relating to AWS node scaling.\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAWS_ENV_FILE\n\n\nLocation of AWS env file used when \nNODE_SCALER_BACKEND\n is sent to \naws\n.\nDefault:\n \n/run/secrets/aws\n\n\n\n\n\n\nAWS_DEFAULT_REGION\n\n\nDefault AWS region.\nDefault:\n \nus-east-1\n\n\n\n\n\n\nAWS_MANAGER_ASG\n\n\nAWS autoscaling group name for manager nodes.\n\n\n\n\n\n\nAWS_WORKER_ASG\n\n\nAWS autoscaling group name for worker nodes.\n\n\n\n\n\n\n\n\nAWS Secrets file\n\u00b6\n\n\nAWS secret file defines the necessary environment variables to authenticate with AWS.\n\n\necho\n \n'export AWS_ACCESS_KEY_ID=xxxx\n\n\nexport AWS_SECRET_ACCESS_KEY=xxxx\n\n\n'\n \n|\n docker secret create aws -",
            "title": "Configuration"
        },
        {
            "location": "/configuration/#configuring-docker-scaler",
            "text": "Docker Scaler  can be configured through Docker enivonment variables and/or by creating a new image based on  thomasjpfan/docker-scaler",
            "title": "Configuring Docker Scaler"
        },
        {
            "location": "/configuration/#service-scaling-environment-variables",
            "text": "Tip  The  Docker Scaler  container can be configured through envionment variables   The following environment variables can be used to configure the  Docker Scaler  relating to service scaling.     Variable  Description      SERVER_PREFIX  Custom prefix for REST endpoint. Default:   /    MIN_SCALE_LABEL  Service label key with value representing the minimum number of replicas. Default:   com.df.scaleMin    MAX_SCALE_LABEL  Service label key with value representing the maximum number of replicas. Default:   com.df.scaleMax    SCALE_DOWN_BY_LABEL  Service label key with value representing the number of replicas to scale down by. Default:   com.df.scaleDownBy    SCALE_UP_BY_LABEL  Service label key with value representing the number of replicas to scale up by. Default:   com.df.scaleUpBy    ALERT_SCALE_MAX  Send alert to alertmanager when trying to scale up service already at max replicas. Default:  true    DEFAULT_MIN_REPLICAS  Default minimum number of replicas for a service. Default:  1    DEFAULT_MAX_REPLICAS  Default maximum number of replicas for a service. Default:  5    DEFAULT_SCALE_SERVICE_DOWN_BY  Default number of replicas to scale service down by. Default:  1    DEFAULT_SCALE_SERVICE_UP_BY  Default number of replicas to scale service up by. Default:  1    ALERTMANAGER_ADDRESS  Address for alertmanager. Default:   http://alertmanager:9093    ALERT_TIMEOUT  Alert timeout duration (seconds). Default:  10    RESCHEDULE_FILTER_LABEL  Services with this label will be rescheduled after node scaling. Default:   com.df.reschedule=true\"    RESCHEDULE_TICKER_INTERVAL  Duration to wait when checking for nodes to come up (seconds). Default:  20    RESCHEDULE_TIMEOUT  Time to wait for nodes to come up during rescheduling (seconds). Default:  1000    RESCHEDULE_ENV_KEY  Key for env variable when rescheduling services. Default:   RESCHEDULE_DATE",
            "title": "Service Scaling Environment Variables"
        },
        {
            "location": "/configuration/#node-scaling-environment-variables",
            "text": "The following environment variables can be used to configure the  Docker Scaler  relating to node scaling.     Variable  Description      NODE_SCALER_BACKEND  Backend of node backend. Accepted Values:  [none, aws] Default:  none    DEFAULT_MIN_MANAGER_NODES  Miniumum number of manager nodes. Default:  3    DEFAULT_MAX_MANAGER_NODES  Maximum number of manager nodes. Default:  7    DEFAULT_MIN_WORKER_NODES  Miniumum number of worker nodes. Default:  1    DEFAULT_MAX_WORKER_NODES  Maximum number of worker nodes. Default:  5",
            "title": "Node Scaling Environment Variables"
        },
        {
            "location": "/configuration/#aws-node-scaling-envronment-variables",
            "text": "The following environment variables can be used to configure the  Docker Scaler  relating to AWS node scaling.     Variable  Description      AWS_ENV_FILE  Location of AWS env file used when  NODE_SCALER_BACKEND  is sent to  aws . Default:   /run/secrets/aws    AWS_DEFAULT_REGION  Default AWS region. Default:   us-east-1    AWS_MANAGER_ASG  AWS autoscaling group name for manager nodes.    AWS_WORKER_ASG  AWS autoscaling group name for worker nodes.",
            "title": "AWS Node Scaling Envronment Variables"
        },
        {
            "location": "/configuration/#aws-secrets-file",
            "text": "AWS secret file defines the necessary environment variables to authenticate with AWS.  echo   'export AWS_ACCESS_KEY_ID=xxxx  export AWS_SECRET_ACCESS_KEY=xxxx  '   |  docker secret create aws -",
            "title": "AWS Secrets file"
        },
        {
            "location": "/feedback-and-contribution/",
            "text": "Feedback and Contribution\n\u00b6\n\n\nThe \nDocker Scaler\n project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are:\n\n\n\n\nCode patches or new features via pull requests\n\n\nDocumentation improvements\n\n\nBug reports and patch reviews\n\n\n\n\nReporting an Issue\n\u00b6\n\n\nFeel fee to \ncreate a new issue\n. Include as much detail as you can.\n\n\nIf an issue is a bug, please provide steps to reproduce it.\n\n\nIf an issue is a request for a new feature, please specify the use-case behind it.\n\n\nContributing To The Project\n\u00b6\n\n\nThis project is developed using \nTest Driven Development\n. When a new feature is added please run through the testing procedure:\n\n\nFork repo\n\u00b6\n\n\ngit clone https://github.com/thomasjpfan/docker-scaler\n\n\n\n\nUnit Testing\n\u00b6\n\n\nmake unit_test\n\n\n\n\nBuild\n\u00b6\n\n\nmake build\n\n\n\n\nTest\n\u00b6\n\n\nmake deploy_test\n\nmake integration_test\n\n\n\n\nCleanup\n\u00b6\n\n\nmake undeploy_test",
            "title": "Feedback and Contribution"
        },
        {
            "location": "/feedback-and-contribution/#feedback-and-contribution",
            "text": "The  Docker Scaler  project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are:   Code patches or new features via pull requests  Documentation improvements  Bug reports and patch reviews",
            "title": "Feedback and Contribution"
        },
        {
            "location": "/feedback-and-contribution/#reporting-an-issue",
            "text": "Feel fee to  create a new issue . Include as much detail as you can.  If an issue is a bug, please provide steps to reproduce it.  If an issue is a request for a new feature, please specify the use-case behind it.",
            "title": "Reporting an Issue"
        },
        {
            "location": "/feedback-and-contribution/#contributing-to-the-project",
            "text": "This project is developed using  Test Driven Development . When a new feature is added please run through the testing procedure:",
            "title": "Contributing To The Project"
        },
        {
            "location": "/feedback-and-contribution/#fork-repo",
            "text": "git clone https://github.com/thomasjpfan/docker-scaler",
            "title": "Fork repo"
        },
        {
            "location": "/feedback-and-contribution/#unit-testing",
            "text": "make unit_test",
            "title": "Unit Testing"
        },
        {
            "location": "/feedback-and-contribution/#build",
            "text": "make build",
            "title": "Build"
        },
        {
            "location": "/feedback-and-contribution/#test",
            "text": "make deploy_test\n\nmake integration_test",
            "title": "Test"
        },
        {
            "location": "/feedback-and-contribution/#cleanup",
            "text": "make undeploy_test",
            "title": "Cleanup"
        },
        {
            "location": "/license/",
            "text": "Docker Scaler License (MIT)\n\u00b6\n\n\nCopyright \u00a9 2017 Thomas Fan\n\n\nThe MIT License (MIT)\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "License"
        },
        {
            "location": "/license/#docker-scaler-license-mit",
            "text": "Copyright \u00a9 2017 Thomas Fan  The MIT License (MIT)  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "Docker Scaler License (MIT)"
        }
    ]
}